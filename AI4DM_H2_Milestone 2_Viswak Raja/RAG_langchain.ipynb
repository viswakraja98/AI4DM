{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader #load the document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter #for creating chunks from the loaded document\n",
    "from langchain_openai import OpenAIEmbeddings #for converting chunks into embeddings\n",
    "from langchain_chroma import Chroma #database for stroring the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\GaTech\\Class Work\\Semester 3\\AI4DM\\Git_H2\\AI4DM_Lab\\AI4DM_H2_Milestone 2_Viswak Raja - Copy\\chroma_db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "dir = os.getcwd()\n",
    "db_dir = os.path.join(dir,\"chroma_db\")\n",
    "print(db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the text content from the .txt file and load it as langchain document\n",
    "loader = TextLoader('emotion.txt')\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document chunk info:\n",
      "\n",
      "Number of document chunks: 247\n",
      "Sample chunk: \n",
      "i guess it doesn t help that i got sick on black friday and was forced against my will to maintain my promise to stay in but being back in the city feels amazing\n",
      "i feel as though i am on another adventure and i am more curious about it than anything else\n",
      "i feel more amazed and more thankful for having e in our lives\n",
      "i still sit back and feel amazed by the whole thing\n",
      "i have been feeling overwhelmed with it all and needing to take time out\n",
      "i replied feeling strange at giving the orders\n",
      "i wonder if the homeowners would feel weird if i parked to gape at their landscaping\n",
      "i both feel impatience at the rate of loss and impressed at the same time\n",
      "i ran errands to buy cora a few newborn sized sleepers i had not previously made any newborn sized babies and went out to lunch to celebrate how great i was feeling i feel amazing no pain no pain meds and moving around almost completely normally at days out\n",
      "i think or feel but like this person i am still amazed by them\n",
      "i feel shame in a strange way\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Split the document into chunks using text splitters \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(document)\n",
    "\n",
    "print(\"Document chunk info:\\n\")\n",
    "print(f\"Number of document chunks: {len(chunks)}\")\n",
    "print(f\"Sample chunk: \\n{chunks[3].page_content}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create embeddings using openAI embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1855c64d880>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store the embeddings and chunks into Chroma DB\n",
    "Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve and generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the DB for retrieval\n",
    "embeddings_used = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorDB = Chroma(persist_directory=db_dir,embedding_function=embeddings_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up Retriver\n",
    "retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRetriever(dir):\n",
    "    \"\"\"\n",
    "    dir is the directory of the vector DB\n",
    "    \"\"\"\n",
    "    embeddings_used = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vectorDB = Chroma(persist_directory=dir,embedding_function=embeddings_used)\n",
    "    retriever = vectorDB.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textGeneration_langChain_RAG(msg,type,retrieverDir, image_data):\n",
    "    \"\"\"\n",
    "    msg is the scenario for the story from the pic (hugging face model output);\n",
    "    type is the emotion of the story- Surprised, Anger, Joy, fear, Sadness\n",
    "    retriever is the vector DB with relevant stories from txt \n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "            model=\"gpt-4o\",\n",
    "            temperature=0.2,\n",
    "            max_tokens=200,\n",
    "            timeout=None,\n",
    "            max_retries=2\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    prmpt = f'You are an expert at describing images for Blind and Low Vision users.Describe this image in {type} genre'\n",
    "    out_message =  HumanMessagePromptTemplate.from_template(\n",
    "        template=[\n",
    "            {\"type\": \"text\", \"text\": prmpt},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": \"{image_data}\",\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([out_message])\n",
    "\n",
    "    \n",
    "    rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    retriever = getRetriever(retrieverDir)\n",
    "\n",
    "    out_message = rag_chain.invoke({\n",
    "        \"story_type\" : type,\n",
    "        \"context\":retriever,\n",
    "        \"image_data\": image_data\n",
    "        # \"scenario_lang\" : msg,\n",
    "    })\n",
    "    \n",
    "    return out_message\n",
    "\n",
    "def runModels_langchain_RAG(url, type, retrieverDir):\n",
    "    image_data = local_image_to_data_url(url)\n",
    "    scenario = img2text(url)\n",
    "    story = textGeneration_langChain_RAG(scenario,type,retrieverDir, image_data)\n",
    "    return([scenario,story])\n",
    "\n",
    "import base64\n",
    "from mimetypes import guess_type\n",
    "\n",
    "# Function to encode a local image into data URL \n",
    "def local_image_to_data_url(image_path):\n",
    "    mime_type, _ = guess_type(image_path)\n",
    "    # Default to png\n",
    "    if mime_type is None:\n",
    "        mime_type = 'image/png'\n",
    "\n",
    "    # Read and encode the image file\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        base64_encoded_data = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "    # Construct the data URL\n",
    "    return f\"data:{mime_type};base64,{base64_encoded_data}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "textGeneration_langChain_RAG() missing 1 required positional argument: 'image_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m scenario \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbookshelves with many different colored books on them in a library\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m#example output from huggingface model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m story \u001b[38;5;241m=\u001b[39m \u001b[43mtextGeneration_langChain_RAG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjoy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(story)\n",
      "\u001b[1;31mTypeError\u001b[0m: textGeneration_langChain_RAG() missing 1 required positional argument: 'image_data'"
     ]
    }
   ],
   "source": [
    "scenario = \"bookshelves with many different colored books on them in a library\" #example output from huggingface model\n",
    "story = textGeneration_langChain_RAG(scenario,\"joy\", db_dir)\n",
    "print(story)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
